{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e0a707-94d5-4326-b3c1-40977ebd1eea",
   "metadata": {},
   "source": [
    "# Testing the web camerA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a24d10-bebb-4121-9782-f0b791b55420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # OpenCV library for webcam and image processing\n",
    "\n",
    "# Open webcam (index 0 is default camera—use 1 or 2 if multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if webcam opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")  # Error if webcam isn’t detected\n",
    "    exit()  # Exit script if no webcam\n",
    "\n",
    "# Loop to show live feed\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read frame: ret is True if successful, frame is image array\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")  # Error if frame capture fails\n",
    "        break  # Exit loop on failure\n",
    "    \n",
    "    cv2.imshow(\"Webcam Test\", frame)  # Display frame in a window named \"Webcam Test\"\n",
    "    \n",
    "    # Wait 1ms for keypress, check for 'q' (ASCII 113) to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit loop on 'q'\n",
    "\n",
    "# Cleanup\n",
    "cap.release()  # Release webcam resource\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24dad97-d04d-4c9b-94ec-2dea2492d455",
   "metadata": {},
   "source": [
    "## Add Hand Detection with Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c2f521-7d05-4945-b844-f5bf453dcf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam opened successfully! Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2  # For webcam and image processing\n",
    "import mediapipe as mp  # For hand detection and landmarks\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands  # Load hands detection module\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)  # Detect 1 hand, 70% confidence threshold\n",
    "mp_draw = mp.solutions.drawing_utils  # Tool to draw hand landmarks\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Webcam opened successfully! Press 'q' to quit.\")  # User feedback\n",
    "\n",
    "# Loop for live hand detection\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    \n",
    "    # Convert frame from BGR (OpenCV) to RGB (Mediapipe needs RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process frame to detect hands\n",
    "    results = hands.process(rgb_frame)  # Returns hand landmarks or None if no hand\n",
    "    \n",
    "    # Draw landmarks if hand detected\n",
    "    if results.multi_hand_landmarks:  # Check if any hands found\n",
    "        for hand_landmarks in results.multi_hand_landmarks:  # Loop through detected hands (1 here)\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)  # Draw green dots and lines\n",
    "    \n",
    "    cv2.imshow(\"Hand Detection\", frame)  # Show frame with landmarks\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()  # Close Mediapipe Hands instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65365925-469e-4d73-bf8c-43acd983cff7",
   "metadata": {},
   "source": [
    "## collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419a4b1-e982-4668-bfd7-d2323a47e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # For webcam and saving images\n",
    "import mediapipe as mp  # For hand detection\n",
    "import os  # For creating directories\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Define new gestures—your choice (e.g., What's up: palm up, Not good: thumbs-down, Good: OK sign)\n",
    "gestures = [\"whats_up\", \"not_good\", \"good\"]\n",
    "for gesture in gestures:\n",
    "    os.makedirs(f\"dataset/{gesture}\", exist_ok=True)  # Create folder for each gesture—overwrites old data\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'w' for What's up, 'n' for Not good, 'g' for Good, 'q' to quit.\")  # Key instructions\n",
    "\n",
    "# Variables for collection\n",
    "current_gesture = None  # Tracks which gesture we’re collecting\n",
    "count = 0  # Counts images per gesture\n",
    "\n",
    "# Loop to collect data\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)  # Mirror feed—makes it intuitive (right hand on right), matches live prediction\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    # Show landmarks for feedback\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    # Display current collection status\n",
    "    if current_gesture:\n",
    "        cv2.putText(frame, f\"Collecting: {current_gesture} ({count})\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Gesture Collection\", frame)\n",
    "    \n",
    "    # Handle key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('w'):  # Start collecting \"What's up\"\n",
    "        current_gesture = \"whats_up\"\n",
    "        count = 0\n",
    "    elif key == ord('n'):  # Start \"Not good\"\n",
    "        current_gesture = \"not_good\"\n",
    "        count = 0\n",
    "    elif key == ord('g'):  # Start \"Good\"\n",
    "        current_gesture = \"good\"\n",
    "        count = 0\n",
    "    elif key == ord('q'):  # Quit\n",
    "        break\n",
    "    \n",
    "    # Save image if collecting and hand detected\n",
    "    if current_gesture and results.multi_hand_landmarks and count < 400:\n",
    "        filename = f\"dataset/{current_gesture}/{count}.jpg\"  # e.g., dataset/whats_up/0.jpg\n",
    "        cv2.imwrite(filename, frame)  # Save frame as JPEG\n",
    "        count += 1  # Increment count\n",
    "        print(f\"Saved {filename}\")  # Confirm save\n",
    "    \n",
    "    # Stop collecting after 400\n",
    "    if count >= 400:\n",
    "        current_gesture = None  # Move to next gesture\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196e29a-dc69-4375-8ddd-c303c29be8dd",
   "metadata": {},
   "source": [
    "## preprocessing the data ,making redy to feed in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c3e92-605a-47e6-8c34-1260be98999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # For loading and processing images\n",
    "import os  # For file operations\n",
    "import numpy as np  # For array conversion\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "\n",
    "# Define parameters\n",
    "IMG_SIZE = (224, 224)  # VGG16 expects 224x224 RGB images\n",
    "gestures = [\"whats_up\", \"not_good\", \"good\"]  # New gestures\n",
    "data_dir = \"dataset\"  # Root folder with images\n",
    "\n",
    "# Lists to store data\n",
    "data = []  # Holds preprocessed images\n",
    "labels = []  # Holds labels (0, 1, 2)\n",
    "\n",
    "# Load and preprocess images\n",
    "for gesture in gestures:\n",
    "    gesture_path = os.path.join(data_dir, gesture)  # e.g., dataset/whats_up\n",
    "    label = gestures.index(gesture)  # 0 = whats_up, 1 = not_good, 2 = good\n",
    "    \n",
    "    for img_file in os.listdir(gesture_path):  # Loop through images (e.g., 0.jpg)\n",
    "        img_path = os.path.join(gesture_path, img_file)  # Full path\n",
    "        img = cv2.imread(img_path)  # Load image in BGR format\n",
    "        if img is None:\n",
    "            print(f\"Failed to load {img_path}\")  # Warn if image fails\n",
    "            continue\n",
    "        \n",
    "        img = cv2.resize(img, IMG_SIZE)  # Resize to 224x224—VGG16 input size\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB—VGG16 expects RGB\n",
    "        img = img / 255.0  # Normalize pixels from 0-255 to 0-1—matches ImageNet pretraining\n",
    "        \n",
    "        data.append(img)  # Add image to list\n",
    "        labels.append(label)  # Add corresponding label\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")  # e.g., (1200, 224, 224, 3)—float32 for TensorFlow\n",
    "labels = np.array(labels)  # e.g., (1200,)—integer labels\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42  # 20% test, fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Training data shape:\", X_train.shape)  # e.g., (960, 224, 224, 3)\n",
    "print(\"Testing data shape:\", X_test.shape)    # e.g., (240, 224, 224, 3)\n",
    "print(\"Training labels shape:\", y_train.shape)  # e.g., (960,)\n",
    "print(\"Testing labels shape:\", y_test.shape)    # e.g., (240,)\n",
    "\n",
    "# Save preprocessed data\n",
    "np.save(\"X_train.npy\", X_train)  # Save training images\n",
    "np.save(\"X_test.npy\", X_test)    # Save testing images\n",
    "np.save(\"y_train.npy\", y_train)  # Save training labels\n",
    "np.save(\"y_test.npy\", y_test)    # Save testing labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

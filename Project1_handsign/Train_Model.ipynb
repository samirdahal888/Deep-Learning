{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03606f0e-47f7-4968-a51f-e982c9dc8d15",
   "metadata": {},
   "source": [
    "## Train the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8317617-c81e-41a9-8c10-6dec6351c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Core deep learning framework\n",
    "from tensorflow.keras import layers, models  # For building the model\n",
    "from tensorflow.keras.applications import VGG16  # Pretrained VGG16 model\n",
    "import numpy as np  # For array operations\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train = np.load(\"X_train.npy\")  # Training images\n",
    "X_test = np.load(\"X_test.npy\")    # Testing images\n",
    "y_train = np.load(\"y_train.npy\")  # Training labels\n",
    "y_test = np.load(\"y_test.npy\")    # Testing labels\n",
    "\n",
    "# Load VGG16 pretrained on ImageNet\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "# weights=\"imagenet\": Use pretrained weights from ImageNet—gives a head start\n",
    "# include_top=False: Exclude final 1000-class layer—we add our own for 3 gestures\n",
    "# input_shape: Matches our 224x224 RGB images\n",
    "\n",
    "# Unfreeze last 10 layers for fine-tuning\n",
    "base_model.trainable = True  # Allow training of base model layers\n",
    "for layer in base_model.layers[:-10]:  # Freeze all but last 10 layers\n",
    "    layer.trainable = False  # Keep early layers fixed—general features like edges\n",
    "\n",
    "# Build model with custom layers\n",
    "model = models.Sequential([\n",
    "    base_model,  # VGG16 base—outputs 7x7x512 feature maps\n",
    "    layers.GlobalAveragePooling2D(),  # Reduce 7x7x512 to 1x512—no parameters, just averaging\n",
    "    layers.Dense(256, activation=\"relu\"),  # 256 neurons—learns gesture-specific patterns\n",
    "    layers.Dropout(0.5),  # Drop 50% of neurons—prevents overfitting to training data\n",
    "    layers.Dense(3, activation=\"softmax\")  # 3 outputs—probabilities for whats_up, not_good, good\n",
    "])\n",
    "\n",
    "# Compute class weights to prevent bias (e.g., \"Yes-only\" issue)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))  # e.g., {0: 1.0, 1: 1.0, 2: 1.0}—adjusts for slight imbalances\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Low rate—fine-tunes gently\n",
    "    loss=\"sparse_categorical_crossentropy\",  # Loss for integer labels (0, 1, 2)\n",
    "    metrics=[\"accuracy\"]  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Show model summary\n",
    "model.summary()  # Displays layers, trainable parameters (~7M), non-trainable (~8M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c8683-1584-4508-bb65-45ffca2b9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to avoid overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5  # Stop if validation loss doesn’t improve for 5 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Training data\n",
    "    epochs=30,  # Max 30 passes—early stopping may halt earlier\n",
    "    batch_size=32,  # 32 images per batch—balances speed and memory\n",
    "    validation_data=(X_test, y_test),  # Validate on test set\n",
    "    callbacks=[early_stopping],  # Apply early stopping\n",
    "    class_weight=class_weight_dict  # Ensure balanced learning across gestures\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")  # e.g., 0.9500—95%\n",
    "\n",
    "# Check prediction distribution\n",
    "predictions = model.predict(X_test)  # Predict on test set\n",
    "pred_labels = np.argmax(predictions, axis=1)  # Get predicted class indices\n",
    "from collections import Counter\n",
    "print(\"Prediction distribution:\", Counter(pred_labels))  # e.g., {0: 80, 1: 80, 2: 80}—should be balanced\n",
    "\n",
    "# Save model in Keras 3 format\n",
    "model.save(\"gesture_model_vgg16_new.keras\")  # Saves as .keras—compatible with TensorFlow 3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3dbd53-9a16-4845-8139-2ec61f502494",
   "metadata": {},
   "source": [
    "## Real Time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d8e21-3a66-444b-9aee-ea1f12e6ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam opened successfully! Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import cv2  # For webcam and image processing\n",
    "import mediapipe as mp  # For hand detection\n",
    "import numpy as np  # For array operations\n",
    "import tensorflow as tf  # For loading model and prediction\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"gesture_model_vgg16_new.keras\")\n",
    "gestures = [\"What's up\", \"Not good\", \"Good\"]  # New gesture labels\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Webcam opened successfully! Press 'q' to quit.\")\n",
    "\n",
    "# Loop for real-time prediction\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)  # Mirror feed—matches training\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    if results.multi_hand_landmarks:  # If hand detected\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Get bounding box coordinates\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)\n",
    "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)\n",
    "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)\n",
    "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)\n",
    "            \n",
    "            # NEW: Fix coordinates to stay inside frame and avoid empty crop\n",
    "            x_min = max(0, x_min)  # Don’t go left of the picture\n",
    "            y_min = max(0, y_min)  # Don’t go above the picture\n",
    "            x_max = min(w, x_max)  # Don’t go right of the picture\n",
    "            y_max = min(h, y_max)  # Don’t go below the picture\n",
    "            \n",
    "            # Crop and preprocess hand region\n",
    "            hand_img = frame[y_min:y_max, x_min:x_max]\n",
    "            # NEW: Check if the crop worked\n",
    "            if hand_img.size > 0:  # Only proceed if we got something\n",
    "                hand_img = cv2.resize(hand_img, (224, 224))  # VGG16 input size\n",
    "                hand_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                hand_img = hand_img / 255.0  # Normalize to 0-1\n",
    "                hand_img = np.expand_dims(hand_img, axis=0)  # Add batch dimension\n",
    "                \n",
    "                # Predict gesture with verbose=0\n",
    "                prediction = model.predict(hand_img, verbose=0)\n",
    "                gesture_idx = np.argmax(prediction)\n",
    "                confidence = prediction[0][gesture_idx]\n",
    "                gesture = gestures[gesture_idx]\n",
    "                \n",
    "                # Display prediction with confidence\n",
    "                text = f\"{gesture} ({confidence:.2f})\"\n",
    "                cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Sign Language Prediction\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"closed\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5d6b4-4b35-4756-ac6c-5c76aabcc847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
